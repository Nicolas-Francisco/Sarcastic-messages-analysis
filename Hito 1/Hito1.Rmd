---
title: "Exploración de datos Hito 1"
output:
  html_document:
    df_print: paged
---

## Importación del dataset descargado localmente
```{r, message=F}
library(tidyverse)
sarcasmo <- read_csv("../../train-balanced-sarcasm.csv")
```

Lo primero es hacer una revisión inicial del dataset para comprender cómo están estructurados los datos. Esto significa, entender cuantos datos son, cuantas columnas, qué describe cada columna, el tipo de datos de las columnas, entre otras cosas.

## Atributos del dataset

Con la función ```head``` podemos hacernos una idea de cómo son los datos, nos muestra los primeros 5 o 6 datos del dataset con los encabezados de cada atributo. Esto es útil para ver si los datos quedaron bien cargados o no.

```{r}
head(sarcasmo)
```

Dimensiones del dataset

```{r}
dim(sarcasmo)
```
La función ```summary``` aplica estadísticas a cada columna. En particular, indica el promedio, mediana, quantiles, calor máximo, mínimo, entre otros. 

```{r}
summary(sarcasmo)
```


## Analisís de datos sobre los comentarios sarcasticos

Un buen punto de partida sería averiguar cuales son las palabras más recurrentes en los comentarios sarcasticos presentes en este dataset. Para ello, haremos una limpieza sobre estos.

Si bien entendemos que los tipos de puntuación utilizados, la diferenciación entre mayúsculas y minúsculas, entre otros elementos que se usan para enriquecer los mensajes escritos son clave a la hora de transmitir el sarcasmo, de momento solo nos interesa la frecuencia de las palabras en si y no otras caracteristicas como por ejemplo el como están escritas.


Dicho esto, la forma más tradicional de representar texto es considerar cada caracter de cada comentario y agregarlo como una columna al dataset. Ya que como se dijo la idea principal es considerar, por ejemplo, si aparece o no una palabra en cierto comentario o cuántas veces aparece en él. Por lo tanto, podríamos tener tantas columnas como elementos tengamos en la colección completa.

Para lograr esto, utilizaremos la librería ``tm`` la cual permite realizar _text mining_ en R.

```{r, message=F}
library(tm)
```

Convertiremos nuestro vector de comentarios sarcasticos a uno que pueda ser leído por ``tm``, donde habrán tantos documentos como comentarios. Luego, crearemos un Corpus o colección de documentos.

```{r}
docs <- VectorSource(sarcasmo[,c("comment")])
docs <- VCorpus(docs)
```

Al ejecutar la siguiente instrucción, veremos resumidamente cómo está compuesto el Corpus o colección de documentos

```{r, eval=F}
inspect(docs)
```



## Pre-procesamiento de texto
En un comienzo, el contenido de cada documento en nuestra colección contendrá mucha información que de momento no es relevante para nosotros. Por ejemplo, puntuaciones, números, palabras no relevantes, etc. Por ello, es necesario efectuar el pre-procesamiento y limpieza de los datos.

### Remover puntuación
```{r}
docs <- tm_map(docs, removePunctuation)
```

### Remover números
```{r}
docs <- tm_map(docs, removeNumbers)
```

### Convertir a minúscula
```{r}
docs <- tm_map(docs, content_transformer(tolower))
```

### Eliminar espacios en blanco innecesarios
```{r}
docs <- tm_map(docs, stripWhitespace)
```

### Reemplazar caracteres específicos
```{r}
docs <- tm_map(docs, content_transformer(gsub), pattern = "/", replacement = "")
docs <- tm_map(docs, content_transformer(gsub), pattern = '[[:digit:]]+', replacement = "")  # elimina cualquier digito
```

### Eliminar tildes (A pesar que los comentarios están en ingles, nunca está de más prevenir)
```{r}
docs <- tm_map(docs, content_transformer(iconv), from="UTF-8",to="ASCII//TRANSLIT")
```

### Remover caracteres especiales no considerados por ```removePunctuation```
```{r}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
docs <- tm_map(docs, content_transformer(removeSpecialChars))
```




## Matriz Documento-término

Dado que necesitamos representar los datos de alguna manera, una forma tradicional de hacerlo es mediante una matriz. La idea principal es considerar cada documento como una fila, la cual a su vez tiene tantas columnas como términos existan en el corpus completo de documento (no por documento). De esta forma, podríamos conocer cuáles términos se repiten entre documentos (por ejemplo).

Para esto, usaremos la función ``DocumentTermMatrix``, que utilizará nuestra colección completa de documentos.


```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```
### Transformar la DocumentTermMatrix en una matriz "visualizable"

Dado que la función ``DocumentTermMatrix`` agrega más información aparte de solo la matriz (metadata), muchas veces nos gustaría visualizar la matriz de forma más sencilla. Para ello, transformaremos el contenido a una matriz.


```{r}
dtm.matrix <- as.matrix(dtm) 
```

### Términos más frecuentes presentes en los comentarios sacasticos

Creamos un dataframe donde tendremos 2 columnas: una para el término y otra para la cantidad de veces que aparece en la colección completa.

```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

### Graficar términos más frecuentes presentes en los comentarios sacasticos

```{r}
library(ggplot2)
```

```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios sarcasticos")
```



Como es posible apreciar, la mayor de estas palabras son aquellas que no entregan mayor significado a los documentos. Por ejemplo, artículos o preposiciones en su mayoría. Para solucionar este problema, podemos considerar una bolsa de palabras comunes llamada ``stopwords``. Por lo tanto y sobre el corpus original de documentos, eliminaremos palabras comunes para luego calcular la matriz nuevamente.

### Removeremos las stopwords:
```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
dtm.sw <- DocumentTermMatrix(docs)
dtm.sw.matrix <- as.matrix(dtm.sw)
freq.sw <- colSums(dtm.sw.matrix)
word_freq.sw <- data.frame(word = names(freq.sw), freq = freq.sw, row.names = NULL)
word_freq.sw <- word_freq.sw[order(-word_freq.sw$freq),]
```

### Volver a graficar términos más frecuentes presentes en los comentarios sarcasticos, esta vez sin considerar stopwords

```{r}
ggplot(word_freq.sw[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios sarcasticos sin considerar stopwords")
```



## Analisís de datos sobre los comentarios padre (misma idea que con los comentarios sarcasticos)

Convertiremos nuestro vector de mensajes de texto a uno que pueda ser leído por ``tm``, donde habrán tantos documentos como comentario padre. Luego, crearemos un Corpus o colección de documentos.

```{r}
docs <- VectorSource(sarcasmo[,c("parent_comment")])
docs <- VCorpus(docs)
inspect(docs)
```

## Pre-procesamiento de texto

### Remover puntuación
```{r}
docs <- tm_map(docs, removePunctuation)
```

### Remover números
```{r}
docs <- tm_map(docs, removeNumbers)
```

### Convertir a minúscula
```{r}
docs <- tm_map(docs, content_transformer(tolower))
```

### Eliminar espacios en blanco innecesarios
```{r}
docs <- tm_map(docs, stripWhitespace)
```

### Reemplazar caracteres específicos
```{r}
docs <- tm_map(docs, content_transformer(gsub), pattern = "/", replacement = "")
docs <- tm_map(docs, content_transformer(gsub), pattern = '[[:digit:]]+', replacement = "")  # elimina cualquier digito
```

### Eliminar tildes
```{r}
docs <- tm_map(docs, content_transformer(iconv), from="UTF-8",to="ASCII//TRANSLIT")
```

### Remover caracteres especiales no considerados por ```removePunctuation```
```{r}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
docs <- tm_map(docs, content_transformer(removeSpecialChars))
```




## Matriz Documento-término
```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

### Transformar la DocumentTermMatrix en una matriz "visualizable"
```{r}
dtm.matrix <- as.matrix(dtm) 
```

### Términos más frecuentes presentes en los comentarios padre
```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

### Graficar términos más frecuentes presentes en los comentarios padre
```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios padre")
```



### Remover stopwords:
```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
dtm.sw <- DocumentTermMatrix(docs)
dtm.sw.matrix <- as.matrix(dtm.sw)
freq.sw <- colSums(dtm.sw.matrix)
word_freq.sw <- data.frame(word = names(freq.sw), freq = freq.sw, row.names = NULL)
word_freq.sw <- word_freq.sw[order(-word_freq.sw$freq),]
```

### Volver a graficar términos más frecuentes presentes en los comentarios padre, esta vez sin considerar stopwords

```{r}
ggplot(word_freq.sw[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios padre sin considerar stopwords")
```