---
html_document: null
subtitle: 'Integrantes: Nicolás García - Javier Lavados - José Triviño - Pablo Gutierrez
  - Sebastián Salinas'
title: "Análisis de mensajes Sarcásticos"
df_print: paged
---

# Introducción
El sarcasmo es una forma de comunicación mediante la cual las personas expresan información que debe ser interpretada con el sentido contrario a su significado literal, usualmente con un fin humorístico o de burla. El sarcasmo es principalmente distinguido a través del tono de voz empleado, y depende en gran parte del contexto en el cual se utiliza. La identificación del sarcasmo en datos de texto es uno de los grandes desafíos en el procesamiento de lenguajes naturales (PLN), el cual se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, en contraposición a los lenguajes de programación.

Reddit es una red social en la cual los usuarios pueden participar subiendo texto, imágenes, videos o enlaces a distintas comunidades organizadas por tema, también conocidas como “subreddits”, que cubren una gran variedad de tópicos como noticias, política, ciencia, películas y videojuegos específicos, etc. Cada post cuenta con su propia sección de comentarios, donde los usuarios pueden opinar sobre el post en cuestión.

Una convención frecuentemente utilizada en esta red social consiste en escribir “/s” al final de un comentario si este debe ser interpretado de forma sarcástica, lo cual sirve para remover cualquier ambigüedad acerca de la naturaleza de este, efectivamente implicando que el usuario desea transmitir el sentimiento opuesto a lo comentado.

El propósito de este estudio consiste en analizar el contenido de un dataset de comentarios sarcásticos de Reddit, utilizando una base de datos con 1.3 millones de comentarios marcados con la etiqueta “/s”, cada uno con el comentario padre correspondiente, contando además con el subreddit del cual fueron extraídos, otorgando así un mayor contexto acerca del tema que está siendo abordado en la conversación.

# Motivación
El procesamiento de lenguajes naturales (abreviado PLN) es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural. Una de las principales dificultades en esta disciplina es la inherente ambigüedad presente en las lenguas naturales, tanto a nivel léxico (una palabra puede tener varios significados distintos) o estructural (construcción de árboles sintácticos). En el nivel pragmático, una oración puede no necesariamente significar lo que realmente se está diciendo, y elementos como el sarcasmo pueden alterar completamente el significado de una oración. 

El sarcasmo como elemento es transmitido principalmente a través del tono de voz empleado, por lo que expresarlo mediante texto puede resultar complicado para los humanos, y aún más complicado puede ser interpretarlo para una máquina.

# Exploración 
El análisis se llevará a cabo con el objetivo de conseguir información importante sobre las conductas de la gente a la hora de escribir sarcasmo. Ejemplos de interrogantes pueden ser:
- ¿Podemos filtrar varios subreddits, cuales usan más sarcasmo relativo al número de suscriptores o de comentarios totales?
- Determinar las palabras más usadas en un comentario sarcástico. (Comparar con base de datos de frecuencia de palabras en inglés)
- Determinar la relevancia de la puntuación y el uso de mayúsculas al caracterizar un comentario sarcástico.

Lo primero es hacer una revisión inicial del dataset para comprender cómo están estructurados los datos. Esto significa, entender cuantos datos son, cuantas columnas, qué describe cada columna, el tipo de datos de las columnas, entre otras cosas:

### Importación del dataset descargado localmente
```{r, message=F}
library(tidyverse)
sarcasmo <- read_csv("../../train-balanced-sarcasm.csv")
```

### Atributos del dataset
Con la función ```head``` podemos hacernos una idea de cómo son los datos, nos muestra los primeros 5 o 6 datos del dataset con los encabezados de cada atributo. Esto es útil para ver si los datos quedaron bien cargados o no.
```{r}
head(sarcasmo)
```

### Dimensiones del dataset
```{r}
dim(sarcasmo)
```
La función ```summary``` aplica estadísticas a cada columna. En particular, indica el promedio, mediana, quantiles, calor máximo, mínimo, entre otros. 
```{r}
summary(sarcasmo)
```

## Exploración de datos sobre los comentarios sarcasticos
Primero se realizó un análisis de las palabras más frecuentes, tanto para los comentarios sarcásticos como para los comentarios padre, los cuales no contienen la etiqueta “/s”. Un buen punto de partida sería averiguar cuales son las palabras más recurrentes en los comentarios sarcasticos presentes en este dataset. Para ello, haremos una limpieza sobre estos.

Si bien entendemos que los tipos de puntuación utilizados, la diferenciación entre mayúsculas y minúsculas, entre otros elementos que se usan para enriquecer los mensajes escritos son clave a la hora de transmitir el sarcasmo, de momento solo nos interesa la frecuencia de las palabras en si y no otras caracteristicas como por ejemplo el como están escritas.

Dicho esto, la forma más tradicional de representar texto es considerar cada caracter de cada comentario y agregarlo como una columna al dataset. Ya que como se dijo la idea principal es considerar, por ejemplo, si aparece o no una palabra en cierto comentario o cuántas veces aparece en él. Por lo tanto, podríamos tener tantas columnas como elementos tengamos en la colección completa.

Para lograr esto, utilizaremos la librería ``tm`` la cual permite realizar _text mining_ en R:
```{r, message=F}
library(tm)
```

Convertiremos nuestro vector de comentarios sarcasticos a uno que pueda ser leído por ``tm``, donde habrán tantos documentos como comentarios. Luego, crearemos un Corpus o colección de documentos.
```{r}
docs <- VectorSource(sarcasmo[,c("comment")])
docs <- VCorpus(docs)
```

Al ejecutar la siguiente instrucción, veremos resumidamente cómo está compuesto el Corpus o colección de documentos
```{r, eval=F}
inspect(docs)
```


### Pre-procesamiento de texto
En un comienzo, el contenido de cada documento en nuestra colección contendrá mucha información que de momento no es relevante para nosotros. Por ejemplo, puntuaciones, números, palabras no relevantes, etc. Por ello, es necesario efectuar el pre-procesamiento y limpieza de los datos.

Es importane chequear si es que existen valores nulos dentro del dataset antes de realizar la exploración sobre este.
Para revisar la presencia de valores inexistentes en el dataset se utilizará la función is.na(), que retorna un valor de verdad dependiendo si el dato es NA o no. Realizaremos la operación suma sobre estos valores booleanos, si la suma resultante es 0 quiere decir que no se encontraron valores en el dataset en que is.na() retornará TRUE, lo que nos indicaría que no hay datos inexistentes en las columnas.

```{r}
# Suma de valores NA en todas las columnas del dataset
sum(is.na(sarcasmo$comment))
sum(is.na(sarcasmo$author))
sum(is.na(sarcasmo$subreddit))
sum(is.na(sarcasmo$score))
sum(is.na(sarcasmo$ups))
sum(is.na(sarcasmo$downs))
sum(is.na(sarcasmo$date))
sum(is.na(sarcasmo$created_utc))
sum(is.na(sarcasmo$parent_comment))


# Mostrar existencia de filas con valores NA (notar que no son los que tiene como mentario el string '0')
sarcasmo[sarcasmo$comment == 0,]
```

Como se puede observar se encontró que existian valores vacíos para la columna de los comentarios sarcasticos. Sin embargo, estos valores estaban presentes en todas las instancias asociadas a los valores NA. Por lo cual, con el objetivo de purificar el dataset, se realizó una limpieza sobre este, eliminando las filas corruptas.

```{r}
sarcasmo=sarcasmo[is.na(sarcasmo$author)==FALSE,]
head(sarcasmo)
```

La función ```summary``` aplica estadísticas a cada columna. En particular, indica el promedio, mediana, quantiles, calor máximo, mínimo, entre otros. 

```{r}
summary(sarcasmo)
```

#### Remover puntuación
```{r}
docs <- tm_map(docs, removePunctuation)
```

#### Remover números
```{r}
docs <- tm_map(docs, removeNumbers)
```

#### Convertir a minúscula
```{r}
docs <- tm_map(docs, content_transformer(tolower))
```

#### Eliminar espacios en blanco innecesarios
```{r}
docs <- tm_map(docs, stripWhitespace)
```

#### Reemplazar caracteres específicos
```{r}
docs <- tm_map(docs, content_transformer(gsub), pattern = "/", replacement = "")
docs <- tm_map(docs, content_transformer(gsub), pattern = '[[:digit:]]+', replacement = "")  # elimina cualquier digito
```

#### Eliminar tildes (A pesar que los comentarios están en ingles, nunca está de más prevenir)
```{r}
docs <- tm_map(docs, content_transformer(iconv), from="UTF-8",to="ASCII//TRANSLIT")
```

#### Remover caracteres especiales no considerados por ```removePunctuation```
```{r}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
docs <- tm_map(docs, content_transformer(removeSpecialChars))
```

### Matriz Documento-término

Dado que necesitamos representar los datos de alguna manera, una forma tradicional de hacerlo es mediante una matriz. La idea principal es considerar cada documento como una fila, la cual a su vez tiene tantas columnas como términos existan en el corpus completo de documento (no por documento). De esta forma, podríamos conocer cuáles términos se repiten entre documentos (por ejemplo).

Para esto, usaremos la función ``DocumentTermMatrix``, que utilizará nuestra colección completa de documentos:
```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

### Transformar la DocumentTermMatrix en una matriz "visualizable"

Dado que la función ``DocumentTermMatrix`` agrega más información aparte de solo la matriz (metadata), muchas veces nos gustaría visualizar la matriz de forma más sencilla. Para ello, transformaremos el contenido a una matriz:

```{r}
dtm.matrix <- as.matrix(dtm) 
```

### Términos más frecuentes presentes en los comentarios sacasticos

Creamos un dataframe donde tendremos 2 columnas: una para el término y otra para la cantidad de veces que aparece en la colección completa:
```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

### Graficar términos más frecuentes presentes en los comentarios sacasticos
Importamos la librería ggplot2 para graficar en R:
```{r}
library(ggplot2)
```

Luego procedemos a graficar nuestro primer análisis, Las palabras más frecuentes:
```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios sarcasticos")
```

Como es posible apreciar, la mayor de estas palabras son aquellas que no entregan mayor significado a los documentos. Por ejemplo, artículos o preposiciones en su mayoría. Para solucionar este problema, podemos considerar una bolsa de palabras comunes llamada ``stopwords``. Por lo tanto y sobre el corpus original de documentos, eliminaremos palabras comunes para luego calcular la matriz nuevamente.

#### Removeremos las stopwords:
```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
dtm.sw <- DocumentTermMatrix(docs)
dtm.sw.matrix <- as.matrix(dtm.sw)
freq.sw <- colSums(dtm.sw.matrix)
word_freq.sw <- data.frame(word = names(freq.sw), freq = freq.sw, row.names = NULL)
word_freq.sw <- word_freq.sw[order(-word_freq.sw$freq),]
```

#### Volver a graficar términos más frecuentes presentes en los comentarios sarcasticos, esta vez sin considerar stopwords

```{r}
ggplot(word_freq.sw[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios sarcasticos sin considerar stopwords")
```



## Exploración de datos sobre los comentarios padre (misma idea que con los comentarios sarcasticos)

Convertiremos nuestro vector de mensajes de texto a uno que pueda ser leído por ``tm``, donde habrán tantos documentos como comentario padre. Luego, crearemos un Corpus o colección de documentos:
```{r}
docs <- VectorSource(sarcasmo[,c("parent_comment")])
docs <- VCorpus(docs)
inspect(docs)
```

### Pre-procesamiento de texto

#### Remover puntuación
```{r}
docs <- tm_map(docs, removePunctuation)
```

####  Remover números
```{r}
docs <- tm_map(docs, removeNumbers)
```

#### Convertir a minúscula
```{r}
docs <- tm_map(docs, content_transformer(tolower))
```

#### Eliminar espacios en blanco innecesarios
```{r}
docs <- tm_map(docs, stripWhitespace)
```

#### Reemplazar caracteres específicos
```{r}
docs <- tm_map(docs, content_transformer(gsub), pattern = "/", replacement = "")
docs <- tm_map(docs, content_transformer(gsub), pattern = '[[:digit:]]+', replacement = "")  # elimina cualquier digito
```

#### Eliminar tildes
```{r}
docs <- tm_map(docs, content_transformer(iconv), from="UTF-8",to="ASCII//TRANSLIT")
```

#### Remover caracteres especiales no considerados por ```removePunctuation```
```{r}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
docs <- tm_map(docs, content_transformer(removeSpecialChars))
```

### Matriz Documento-término
```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

#### Transformar la DocumentTermMatrix en una matriz "visualizable"
```{r}
dtm.matrix <- as.matrix(dtm) 
```

#### Términos más frecuentes presentes en los comentarios padre
```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

## Graficamos términos más frecuentes presentes en los comentarios padre
```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios padre")
```

#### Remover stopwords:
```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
dtm.sw <- DocumentTermMatrix(docs)
dtm.sw.matrix <- as.matrix(dtm.sw)
freq.sw <- colSums(dtm.sw.matrix)
word_freq.sw <- data.frame(word = names(freq.sw), freq = freq.sw, row.names = NULL)
word_freq.sw <- word_freq.sw[order(-word_freq.sw$freq),]
```

## Volver a graficar términos más frecuentes presentes en los comentarios padre, esta vez sin considerar stopwords

```{r}
ggplot(word_freq.sw[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios padre sin considerar stopwords")
```

## Analisis de los comentarios sarcasticos a través de los años (2009-2016)
```{r}
# Obtener frecuencia de los años de los comentarios sarcasticos
anhos=substr(sarcasmo$date,1,4)
anhos=table(anhos)
anhos=data.frame(anhos)

```


```{r}
barplot(anhos$Freq, names.arg = anhos$anhos, main="Comentarios sarcasticos a través del tiempo")

```


Se puede observar que la cantidad de comentarios sarcasticos a través de los años ha tenido una evolución exponencial, lo cual hace mucho sentido por el hecho de que reddit es una plataforma social que ha ido creciendo con el tiempo, acumulando un monton de usuarios y recibiendo nuevos todos los días, llegando a contar con más de 52 millones de usuarios activos diarios a nivel mundial.

## Subreddits más recurrentes en los comentarios sarcasticos del dataset


Observando la columna "subreddit" es posible ver que ciertos tópicos se repiten, es por esto que una buena idea sería realizar una exploración de datos para cuantificar cuales subreddits son los más repetidos en el dataset. Es deci, cuantificar la cantidad de comentarios sarcasticos que tiene cada subreddit presente en el dataset, lo que es equivalente a encontrar los subreddits que son más propensos a tener comentarios sarcasticos.

Convertiremos nuestro vector de subreddits a uno que pueda ser leído por ``tm``, donde habrán tantos documentos como subreddits hayan. Luego, crearemos un Corpus o colección de documentos.

```{r}
docs <- VectorSource(sarcasmo[,c("subreddit")])
docs <- VCorpus(docs)
inspect(docs)
```



## Matriz Documento-término

```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

### Transformar la DocumentTermMatrix en una matriz "visualizable"

```{r}
dtm.matrix <- as.matrix(dtm) 
```

### subreddits más presentes en el dataset
```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

### Graficar subreddits más frecuentes en los comentarios sarcasticos del dataset
```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 subreddits presentes en el dataset")
```


Podemos notar que los tópicos de los subreddits con más interacciones de comentarios sarcasticos son bien variados, y que van desde temas serios como política o noticias hasta temas más ludicos como league of legends o deportes.

Es por esto que una pregunta super valida en este contexto es si los subreddits con mas interacciones de comentarios sarcasticos son los mismos que tienen mayor score a nivel global en sus comentarios (la suma de los scores de sus comentarios sarcasticos)

## Subreddits con el mejor score

```{r}


porTopico <- sarcasmo %>%  
group_by(subreddit) %>% # Agrupar los subreddit                     
summarise(total = sum(score)) %>% # ordenados de menor a mayor por el score de sus comentarios sarcasticos      
arrange(-total) # Ordenarlos de mayor a menor

porTopico = porTopico[0:20,] # 20 subreddits con más score en sus comentarios sarcasticos
porTopico

```
### Graficar subreddits con el mejor score

```{r}


ggplot(data = porTopico, aes(x = reorder(subreddit, total), y=total)) + geom_bar(stat = "identity")+coord_flip()+
          ggtitle(label = "Top-20 subreddits con el mejor score")
```


Notamos que si bien aparecen los mismos subreddits, el orden de aparición cambia, lo que quiere decir que no necesariamente los subreddit con mayor participación de comentarios sarcasticos son los que se llevan mejor score. Esto nos puede dar un indicio de que los comentarios sarcasticos gustan más o no dependiendo del contexto en que se encuentre o sobre el tema que traten.


## Cantidad de carácteres por comentario sarcastico

Otra exploración que podemos realizar y la cual posiblemente nos ayude a encontrar una relación entre si un comentario es sarcastico o no, es identificar la cantidad de caracteres que posee, o dicho de otra forma, encontrar el largo del comentario, además de también conocer estadísticas sobre el largo en palabras de los comentarios, tales como el promedio, desviación estándar, máximo y mínimo..

```{r}
Length_comment=str_length(sarcasmo$comment)
hist(Length_comment,xlim=c(0,1000), breaks = 1000, main = "Cantidad de carácteres por comentario sarcastico")


# Resumen de estadísticas del largo de los comentarios sarcasticos
summary(Length_comment)
```
## Cantidad de carácteres por comentario padre

```{r}
Lenght_parent=str_length(sarcasmo$parent_comment)
hist(Lenght_parent,xlim=c(0,1000), breaks = 1000, main = "Cantidad de carácteres por comentario padre")

# Resumen de estadísticas del largo de los comentarios padre
summary(Lenght_parent)
```


Realizando una compración entre los dos graficos es posible observar que los comentarios sarcasticos se encuentran en un rango entre 0 a 400 caracteres con un promedio de 56.69 caracteres. Por otro lado, los comentarios padres se encuentran en un rango entre 0 y un poco más de 1000 caracteres y con un promedio de 133.4 caractes, lo que nos dice en general que los comentarios padre son más largos que los comentarios sarcasticos. Sin embargo para ambos tipos de comentario, la mayor concentración de datos está entre los 0 a 200 caracteres.


## Conclusiones a partir de la exploración de Datos
Como se puede notar, las palabras resultan bastante similares en frecuencia para ambos tipos de comentario. Esto nos da distintas posibilidades respecto a los comentarios sarcásticos, donde esto podría indicarnos que la mayoría de comentarios sarcásticos suelen mencionar las mismas palabras del comentario padre (Ya sea por énfasis, etc.) o también indicar que algunos comentarios sarcásticos son respuestas de otros comentarios sarcásticos, lo cuál podría dificultar el estudio de comentarios sarcásticos.

A partir de esto se puede intentar buscar la correlación de la presencia de tales palabras en un comentario sarcástico, y ver si existe una doble correlación, es decir, que un comentario sarcástico implique la presencia de ciertas palabras, y que se tenga lo inverso, es decir, la presencia de esta palabra resulte indicativa de un comentario sarcástico. Se podría buscar la presencia de estas palabras en ambos tipos de comentarios y después intentar entrenar un clasificador para ver si en verdad puede predecir la naturaleza del comentario.

Otro análisis que se llevó a cabo fue el del uso de la puntuación y las mayúsculas. Pese a que se eliminaron puntuaciones, letras mayúsculas, y otros strings que entorpezcan el estudio de datos, notamos que gran parte de los comentarios del dataset utilizan muy frecuentemente estos strings con el fin de "exagerar" o "enfatizar" el sarcasmo de los comentarios estudiados. Es necesario realizar un estudio más exaustivo de estos tipos de mensajes, y ver si existe una relación entre cantidad de mayúsculas y otras puntuaciones dentro de un mensaje con sarcásmo.


# Preguntas y problemas

A partir de los datos estudiados se podrían responder interrogantes más complejas como:

- ¿Se puede predecir un comentario sarcástico sin un indicio explícito de la naturaleza de este comentario (sin la existecia del tag “/s”)?
- ¿Existen ciertas características comunes compartidas entre comentarios redactados de forma sarcástica?
- ¿Qué tan necesario es conocer el contexto de un comentario para asumir que este es sarcástico?
- ¿Se puede obtener de forma aislada o es necesario conocer el comentario al que se respondía?
- Cantidad de sarcasmo proporcional a los usuarios según subreddit (cuales subreddits dan lugar a discusiones más tóxicas?)