---
html_document: null
subtitle: 'Integrantes: Nicolás García -  Karen Lavados - José Triviño - Pablo Gutierrez
  - Sebastián Salinas'
title: "Análisis de mensajes Sarcásticos"
df_print: paged
---

# Introducción
El sarcasmo es una forma de comunicación mediante la cual las personas expresan información que debe ser interpretada con el sentido contrario a su significado literal, usualmente con un fin humorístico o de burla. El sarcasmo es principalmente distinguido a través del tono de voz empleado, y depende en gran parte del contexto en el cual se utiliza. La identificación del sarcasmo en datos de texto es uno de los grandes desafíos en el procesamiento de lenguajes naturales (PLN), el cual se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural, en contraposición a los lenguajes de programación.

Reddit es una red social en la cual los usuarios pueden participar subiendo texto, imágenes, videos o enlaces a distintas comunidades organizadas por tema, también conocidas como “subreddits”, que cubren una gran variedad de tópicos como noticias, política, ciencia, películas, videojuegos específicos, etc. Cada post cuenta con su propia sección de comentarios, donde los usuarios pueden opinar sobre el post en cuestión.

Una convención frecuentemente utilizada en esta red social consiste en escribir “/s” al final de un comentario si este debe ser interpretado de forma sarcástica, lo cual sirve para remover cualquier ambigüedad acerca de la naturaleza de este, efectivamente implicando que el usuario desea transmitir el sentimiento opuesto a lo comentado.

El propósito de este estudio consiste en analizar el contenido de un dataset de comentarios sarcásticos de Reddit, utilizando una base de datos con 1.3 millones de comentarios marcados con la etiqueta “/s”, cada uno con el comentario padre correspondiente, contando además con el subreddit del cual fueron extraídos, otorgando así un mayor contexto acerca del tema que está siendo abordado en la conversación.

# Motivación
El procesamiento de lenguajes naturales (abreviado PLN) es un campo de las ciencias de la computación, de la inteligencia artificial y de la lingüística que estudia las interacciones entre las computadoras y el lenguaje humano. Se ocupa de la formulación e investigación de mecanismos eficaces computacionalmente para la comunicación entre personas y máquinas por medio del lenguaje natural. Una de las principales dificultades en esta disciplina es la inherente ambigüedad presente en las lenguas naturales, tanto a nivel léxico (una palabra puede tener varios significados distintos) o estructural (construcción de árboles sintácticos). En el nivel pragmático, una oración puede no necesariamente significar lo que realmente se está diciendo, y elementos como el sarcasmo pueden alterar completamente el significado de una oración. 

El sarcasmo como elemento es transmitido principalmente a través del tono de voz empleado, por lo que expresarlo mediante texto puede resultar complicado para los humanos, y aún más complicado puede ser interpretarlo para una máquina. El propósito de este análisis es realizar un estudio donde se puedan encontrar patrones que permitan caracterizar un comentario sarcástico de otros que no lo son, obteniendo información importante para deducir como una máquina podría en un futuro detectar sarcasmo.

# Exploración 
El análisis se llevará a cabo con el objetivo de conseguir información importante sobre las conductas de la gente a la hora de escribir sarcasmo. Ejemplos de interrogantes pueden ser:

- ¿Podemos filtrar varios subreddits, cuales usan más sarcasmo relativo al número de suscriptores o de comentarios totales?

- Determinar las palabras más usadas en un comentario sarcástico. (Comparar con base de datos de frecuencia de palabras en inglés)

- Determinar la relevancia de la puntuación y el uso de mayúsculas al caracterizar un comentario sarcástico.

Lo primero que se realizó fue una revisión inicial del dataset para comprender cómo están estructurados los datos. Esto significa, entender cuantos datos son, cuantas columnas, qué describe cada columna, el tipo de datos de las columnas, entre otras cosas:

### Importación del dataset descargado localmente
```{r, message=F}
library(tidyverse)
sarcasmo <- read_csv("../../train-balanced-sarcasm.csv")
```

### Atributos del dataset
Con la función ```head``` podemos hacernos una idea de cómo son los datos, nos muestra los primeros datos del dataset con los encabezados de cada atributo. Esto es útil para ver si los datos quedaron bien cargados o no.
```{r}
head(sarcasmo)
```

Es importane chequear si es que existen valores nulos dentro del dataset antes de realizar la exploración sobre este.
Para revisar la presencia de valores inexistentes en el dataset se utilizará la función is.na(), que retorna un valor de verdad dependiendo si el dato es NA o no. Realizaremos la operación suma sobre estos valores booleanos, si la suma resultante es 0 quiere decir que no se encontraron valores en el dataset en que is.na() retornará TRUE, lo que nos indicaría que no hay datos inexistentes en las columnas.

```{r}
# Suma de valores NA en todas las columnas del dataset
sum(is.na(sarcasmo$comment))
sum(is.na(sarcasmo$author))
sum(is.na(sarcasmo$subreddit))
sum(is.na(sarcasmo$score))
sum(is.na(sarcasmo$ups))
sum(is.na(sarcasmo$downs))
sum(is.na(sarcasmo$date))
sum(is.na(sarcasmo$created_utc))
sum(is.na(sarcasmo$parent_comment))


# Mostrar existencia de filas con valores NA (notar que no son las que tienen como mentario el string '0', si no las que tienen datos NA)
sarcasmo[sarcasmo$comment == 0,]
```

Como se puede observar se encontró que existian valores vacíos. Por lo cual, con el objetivo de purificar el dataset, se realizó una limpieza sobre este, eliminando las filas corruptas.

```{r}
sarcasmo=sarcasmo[is.na(sarcasmo$author)==FALSE,]
head(sarcasmo)
```

### Dimensiones del dataset
```{r}
dim(sarcasmo)
```

La función ```summary``` aplica estadísticas a cada columna. En particular, indica el promedio, mediana, quantiles, máximo, mínimo, entre otros. 

```{r}
summary(sarcasmo)
```

## Exploración de datos sobre los comentarios sarcasticos
El punto de partida para la exploración de datos fue averiguar cuales eran las palabras más recurrentes en los comentarios sarcasticos y en los comentarios padre, los cuales no contienen la etiqueta “/s”.

Dicho esto, la forma más tradicional de representar texto es considerar cada palabra de cada comentario y agregarlo como una columna al dataset. Ya que como se dijo la idea principal es considerar, por ejemplo, si aparece o no una palabra en cierto comentario o cuántas veces aparece en él.

Utilización de la librería ``tm``, la cual permite realizar _text mining_ en R:
```{r, message=F}
library(tm)
```

Convertir vector de comentarios sarcasticos en uno que pueda ser leído por ``tm``, donde habrán tantos documentos como comentarios. Luego, se crea un Corpus o colección de documentos.
```{r}
docs <- VectorSource(sarcasmo[,c("comment")])
docs <- VCorpus(docs)
```

Al ejecutar la siguiente instrucción, se verá resumidamente cómo está compuesto el Corpus o colección de documentos
```{r, eval=F}
inspect(docs)
```

### Pre-procesamiento de texto
En un comienzo, el contenido de cada documento de la colección contendrá mucha información que de momento no es relevante, ya que Si bien los tipos de puntuación, la diferenciación entre mayúsculas y minúsculas, entre otros elementos que se usan para enriquecer los mensajes escritos son clave a la hora de transmitir el sarcasmo, para esta primera exploración solo se consideró la frecuencia de las palabras y no otras caracteristicas como por ejemplo el cómo están escritas. Por ello, es necesario efectuar el pre-procesamiento y limpieza de los datos.

#### Remover puntuación
```{r}
docs <- tm_map(docs, removePunctuation)
```

#### Remover números
```{r}
docs <- tm_map(docs, removeNumbers)
```

#### Convertir a minúscula
```{r}
docs <- tm_map(docs, content_transformer(tolower))
```

#### Eliminar espacios en blanco innecesarios
```{r}
docs <- tm_map(docs, stripWhitespace)
```

#### Reemplazar caracteres específicos
```{r}
docs <- tm_map(docs, content_transformer(gsub), pattern = "/", replacement = "")
docs <- tm_map(docs, content_transformer(gsub), pattern = '[[:digit:]]+', replacement = "")  # elimina cualquier digito
```

#### Eliminar tildes (A pesar que los comentarios están en inglés, nunca está de más prevenir)
```{r}
docs <- tm_map(docs, content_transformer(iconv), from="UTF-8",to="ASCII//TRANSLIT")
```

#### Remover caracteres especiales no considerados por ```removePunctuation```
```{r}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
docs <- tm_map(docs, content_transformer(removeSpecialChars))
```

### Matriz Documento-término

Dado que se necesita representar los datos de alguna manera, una forma tradicional de hacerlo es mediante una matriz. La idea principal es considerar cada documento como una fila, la cual a su vez tiene tantas columnas como términos existan en el corpus completo de documento (no por documento). De esta forma, se puede conocer cuáles términos se repiten entre documentos.

Para esto, usaremos la función ``DocumentTermMatrix``, que utilizará nuestra colección completa de documentos:
```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

### Transformar la DocumentTermMatrix en una matriz "visualizable"
```{r}
dtm.matrix <- as.matrix(dtm) 
```

### Términos más frecuentes presentes en los comentarios sacasticos

Se creó un dataframe el cual contiene 2 columnas: una para el término y otra para la cantidad de veces que aparece en la colección completa:
```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

Importación de la librería ggplot2 para graficar en R:
```{r}
library(ggplot2)
```

### Graficar términos más frecuentes presentes en los comentarios sarcasticos
```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios sarcasticos") + xlab("Palabras") + ylab("Frecuencia")
```

Como es posible apreciar, la mayor de estas palabras son aquellas que no entregan mayor significado a los documentos. Por ejemplo, artículos o preposiciones en su mayoría. Para solucionar este problema, podemos considerar una bolsa de palabras comunes llamada ``stopwords``. Por lo tanto y sobre el corpus original de documentos, se eliminarán palabras comunes para luego calcular la matriz nuevamente.

#### Remover stopwords:
```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
dtm.sw <- DocumentTermMatrix(docs)
dtm.sw.matrix <- as.matrix(dtm.sw)
freq.sw <- colSums(dtm.sw.matrix)
word_freq.sw <- data.frame(word = names(freq.sw), freq = freq.sw, row.names = NULL)
word_freq.sw <- word_freq.sw[order(-word_freq.sw$freq),]
```

#### Volver a graficar términos más frecuentes presentes en los comentarios sarcasticos, esta vez sin considerar stopwords

```{r}
ggplot(word_freq.sw[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios sarcasticos sin considerar stopwords")  + xlab("Palabras") + ylab("Frecuencia")
```



## Exploración de datos sobre los comentarios padre (misma idea que con los comentarios sarcasticos)

```{r}
docs <- VectorSource(sarcasmo[,c("parent_comment")])
docs <- VCorpus(docs)
inspect(docs)
```

### Pre-procesamiento de texto

#### Remover puntuación
```{r}
docs <- tm_map(docs, removePunctuation)
```

####  Remover números
```{r}
docs <- tm_map(docs, removeNumbers)
```

#### Convertir a minúscula
```{r}
docs <- tm_map(docs, content_transformer(tolower))
```

#### Eliminar espacios en blanco innecesarios
```{r}
docs <- tm_map(docs, stripWhitespace)
```

#### Reemplazar caracteres específicos
```{r}
docs <- tm_map(docs, content_transformer(gsub), pattern = "/", replacement = "")
docs <- tm_map(docs, content_transformer(gsub), pattern = '[[:digit:]]+', replacement = "")  # elimina cualquier digito
```

#### Eliminar tildes
```{r}
docs <- tm_map(docs, content_transformer(iconv), from="UTF-8",to="ASCII//TRANSLIT")
```

#### Remover caracteres especiales no considerados por ```removePunctuation```
```{r}
removeSpecialChars <- function(x) gsub("[^a-zA-Z0-9 ]","",x)
docs <- tm_map(docs, content_transformer(removeSpecialChars))
```

### Matriz Documento-término
```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

### Transformar la DocumentTermMatrix en una matriz "visualizable"
```{r}
dtm.matrix <- as.matrix(dtm) 
```

#### Términos más frecuentes presentes en los comentarios padre
```{r}
freq <- colSums(dtm.matrix)
word_freq <- data.frame(word = names(freq), freq = freq, row.names = NULL)
word_freq <- word_freq[order(-word_freq$freq),]
```

### Graficamos términos más frecuentes presentes en los comentarios padre
```{r}
ggplot(word_freq[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios padre") + xlab("Palabras") + ylab("Frecuencia")
```

#### Remover stopwords:
```{r}
docs <- tm_map(docs, removeWords, stopwords("english"))
```

```{r}
dtm.sw <- DocumentTermMatrix(docs)
dtm.sw.matrix <- as.matrix(dtm.sw)
freq.sw <- colSums(dtm.sw.matrix)
word_freq.sw <- data.frame(word = names(freq.sw), freq = freq.sw, row.names = NULL)
word_freq.sw <- word_freq.sw[order(-word_freq.sw$freq),]
```

### Volver a graficar términos más frecuentes presentes en los comentarios padre, esta vez sin considerar stopwords

```{r}
ggplot(word_freq.sw[1:20,], aes(x = reorder(word, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 palabras presentes en comentarios padre sin considerar stopwords") + xlab("Palabras") + ylab("Frecuencia")
```

Haciendo una comparación entre los graficos sin stopwords para ambos tipos de comentarios, se puede notar que las palabras más utilizadas resultan ser bastante similares (a excepción de un par, como la expresión "yeah", que toma más relevancia en comentarios sarcasticos), y que por el lado de las frecuencias, si bien se aprecian leves cambios en el orden, en general son similares en frecuencia para ambos tipos de comentario.

Esto nos da distintas posibilidades respecto a los comentarios sarcásticos, ya que podría indicarnos que la mayoría de comentarios sarcásticos suelen mencionar las mismas palabras que contiene el comentario padre (Ya sea por énfasis, burla, etc.) o también indicar que algunos comentarios sarcásticos son respuestas de otros comentarios sarcásticos, lo cuál podría dificultar el estudio de este tipo de comentarios.

A partir de esto se puede intentar buscar una correlación entre palabras en un comentario y la clase de este (si es sarcástico o no). O incluso investigar si es que existe una doble correlación, es decir, que un comentario sarcástico implique la presencia de ciertas palabras, y que se tenga lo inverso, que la presencia de ciertas palabras resulte indicativa de un comentario sarcástico. Se podría buscar la presencia de estas palabras en ambos tipos de comentarios y después entrenar un clasificador para ver si en verdad puede predecir la naturaleza del comentario.

Otra conclusión que se puede hacer es sobre el uso de la puntuación y las mayúsculas. Pese a que se eliminaron puntuaciones, letras mayúsculas, y otros strings que entorpezcan el estudio de datos, notamos que gran parte de los comentarios del dataset utilizan muy frecuentemente estos strings con el fin de "exagerar" o "enfatizar" el sarcasmo de los comentarios estudiados. Es necesario realizar un estudio más exaustivo de estos tipos de mensajes, y ver si existe una relación entre cantidad de mayúsculas y otras puntuaciones dentro de un mensaje con sarcásmo.


## Analisis de los comentarios sarcasticos a través de los años (2009-2016)

### Breve intro a la exploracion de cantidad de comentarios a través de los años

```{r}
# Obtener frecuencia de los años de los comentarios sarcasticos
anhos=substr(sarcasmo$date,1,4)
anhos=table(anhos)
anhos=data.frame(anhos)

```


```{r}
barplot(anhos$Freq, names.arg = anhos$anhos, main="Comentarios sarcasticos a través del tiempo", xlab = "Años", ylab = "Frecuencia")

```


Se puede observar que la cantidad de comentarios sarcasticos a través de los años ha tenido una evolución exponencial, lo cual hace mucho sentido por el hecho de que reddit es una plataforma social que ha ido creciendo con el tiempo, acumulando una gran cantidad de usuarios y que sigue recibiendo gente nueva todos los días, llegando a contar con más de 52 millones de usuarios activos diarios a nivel mundial.



## Subreddits más recurrentes en los comentarios sarcasticos del dataset


Observando la columna "subreddit" es posible ver que ciertos tópicos se repiten, es por esto que una buena idea sería realizar una exploración de datos para cuantificar cuales subreddits son los más repetidos en el dataset. Es decir, cuantificar la cantidad de comentarios sarcasticos que tiene cada subreddit presente en el dataset, lo que es equivalente a encontrar los subreddits que son más propensos a tener comentarios sarcasticos.


```{r}
docs <- VectorSource(sarcasmo[,c("subreddit")])
docs <- VCorpus(docs)
inspect(docs)
```



### Matriz Documento-término

```{r}
dtm <- DocumentTermMatrix(docs)
inspect(dtm)
```

### Transformar la DocumentTermMatrix en una matriz "visualizable"

```{r}
dtm.matrix <- as.matrix(dtm) 
```

### subreddits más presentes en el dataset
```{r}
freq <- colSums(dtm.matrix)
subreddit_freq <- data.frame(subreddit = names(freq), freq = freq, row.names = NULL)
subreddit_freq <- subreddit_freq[order(-subreddit_freq$freq),]
subreddit_freq
```

### Graficar subreddits más frecuentes en los comentarios sarcasticos del dataset
```{r}
ggplot(subreddit_freq[1:20,], aes(x = reorder(subreddit, freq), y = freq)) +
          geom_bar(stat = "identity") + 
          coord_flip()+
          ggtitle(label = "Top-20 subreddits presentes en el dataset")  + xlab("Subreddit") + ylab("Frecuencia")
```


Podemos notar que los tópicos de los subreddits con más interacciones de comentarios sarcasticos son bien variados, y que van desde temas serios como política o noticias hasta temas más ludicos como league of legends o deportes.

Es por esto que una pregunta super valida en este contexto es si los subreddits con mas interacciones de comentarios sarcasticos son los mismos que tienen mayor score a nivel global en sus comentarios (la suma de los scores de sus comentarios sarcasticos)

## Subreddits con el mejor score

```{r}
porTopico <- sarcasmo %>%  
group_by(subreddit) %>% # Agrupar los subreddit                     
summarise(total = sum(score)) %>% # ordenados de menor a mayor por el score de sus comentarios sarcasticos      
arrange(-total) # Ordenarlos de mayor a menor

porTopico = porTopico[0:20,] # 20 subreddits con más score en sus comentarios sarcasticos
porTopico

```
### Graficar subreddits con el mejor score

```{r}

ggplot(data = porTopico, aes(x = reorder(subreddit, total), y=total)) + geom_bar(stat = "identity")+coord_flip()+
          ggtitle(label = "Top-20 subreddits con el mejor score")  + xlab("Subreddit") + ylab("Score")
```


Notamos que si bien aparecen los mismos subreddits, el orden de aparición cambia, lo que quiere decir que no necesariamente los subreddit con mayor participación de comentarios sarcasticos son los que se llevan mejor score. Esto nos puede dar un indicio de que los comentarios sarcasticos gustan más o no dependiendo del contexto en que se encuentre o sobre el tema que traten.


## Cantidad de caracteres por comentario sarcastico

Otra exploración que podemos realizar y la cual posiblemente nos ayude a encontrar una relación entre si un comentario es sarcastico o no, es identificar la cantidad de caracteres que posee, o dicho de otra forma, encontrar el largo del comentario, además de también conocer estadísticas sobre el largo en palabras de los comentarios, tales como el promedio, el máximo y mínimo.

```{r}
Length_comment=str_length(sarcasmo$comment)
hist(Length_comment,xlim=c(0,1000), breaks = 1000, main = "Cantidad de caracteres por comentario sarcastico", 
     xlab = "Cantidad de caracteres", ylab = "Frecuencia" )


# Resumen de estadísticas del largo de los comentarios sarcasticos
summary(Length_comment)

sarcasmo[str_length(sarcasmo$comment) == 10000,]
```
## Cantidad de carácteres por comentario padre

```{r}
Lenght_parent=str_length(sarcasmo$parent_comment)
hist(Lenght_parent,xlim=c(0,1000), breaks = 1000, main = "Cantidad de caracteres por comentario padre",
     xlab = "Cantidad de caracteres", ylab = "Frecuencia" )

# Resumen de estadísticas del largo de los comentarios padre
summary(Lenght_parent)
```

Analizando las estadísticas sobre el largo de ambos tipos de comentarios podemos notar la presencia de outliers, ya que en el caso de los comentarios sarcasticos se tiene un promedio de 56,69 caracteres y una mediana de 46 caracteres, pero un máximo de 10000.
Esto nos dice que existe al menos un comentario sarcastico que contiene 10.000 caracteres. Realizando la consulta adecuada podemos observar que son tres los comentarios que alcanzan los 10000 caracteres, y uno de ellos es una secuencia que repite el nombre Donald Trump una gran cantidad de veces, llegando a generar un comentario de 10000 caracteres.

Este comentario se aleja excesivamente de las medidas de tendencia central y por ende es un outlier que afecta a estadisticas que no son robustas, como el promedio. Algo similar pasa en el caso de los comentarios padres, donde se tiene un promedio de 133.4 caractes y una mediana de 75 caracteres, pero un máximo de 40301.

Es por esto que con el objetivo de facilitar la visualización de los datos se realizó un zoom a los dos graficos generados, ya que debido a la presencia de outliers las escalas de los graficos quedaban muy extensas.


Haciendo una compración entre los dos graficos es posible observar que los comentarios sarcasticos se encuentran en un rango entre 0 a 400 caracteres, con una mediana de 46 caracteres. Y que los comentarios padres se encuentran en un rango entre 0 y un poco más de 1000 caracteres, con una mediana de 75 caracteres. Lo que nos dice que en general los comentarios padre son más largos que los comentarios sarcasticos. Sin embargo para ambos tipos de comentario, la mayor concentración de datos está entre los 0 a 200 caracteres.



# Preguntas y problemas

A partir de los datos estudiados se podrían responder interrogantes más complejas como:

- ¿Se puede predecir un comentario sarcástico sin un indicio explícito de la naturaleza de este comentario (sin la existecia del tag “/s”)?
- ¿Existen ciertas características comunes compartidas entre comentarios redactados de forma sarcástica?
- ¿Qué tan necesario es conocer el contexto de un comentario para asumir que este es sarcástico?
- ¿Se puede obtener de forma aislada o es necesario conocer el comentario al que se respondía?
- Cantidad de sarcasmo proporcional a los usuarios según subreddit (cuales subreddits dan lugar a discusiones más tóxicas?)