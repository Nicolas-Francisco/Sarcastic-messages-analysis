{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copia de scikit-learn-text-classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSPCQGv3vEC-"
      },
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.naive_bayes import GaussianNB  # Naive bayes\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qelb5fqiHETC"
      },
      "source": [
        "#Exploracion de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qjx3yECb_jM"
      },
      "source": [
        "Se cargan los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1ZJesL5zQII",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "0ca8a7e8-a1fc-44ea-b7d7-22f5ec62b7f2"
      },
      "source": [
        "yes_or_no = pd.read_excel(\"DatosHito2.xlsx\")\n",
        "yes_or_no.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>Llave</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I don't pay attention to her, but as long as s...</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td></td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>65674.0</td>\n",
              "      <td>0.724415</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sarcasm</td>\n",
              "      <td>170516.0</td>\n",
              "      <td>0.577594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trick or treating in general is just weird...</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AskRedditno</td>\n",
              "      <td>24984.0</td>\n",
              "      <td>0.275585</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nosarcasm</td>\n",
              "      <td>124702.0</td>\n",
              "      <td>0.422406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what the fuck</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>22</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>90658.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>295218.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This would make me cry.</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My stuffed animal I've had since I was born.</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>funnyyes</td>\n",
              "      <td>17939.0</td>\n",
              "      <td>0.418149</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  ... Unnamed: 12\n",
              "0  I don't pay attention to her, but as long as s...  ...    0.577594\n",
              "1      Trick or treating in general is just weird...  ...    0.422406\n",
              "2                                      what the fuck  ...         NaN\n",
              "3                            This would make me cry.  ...         NaN\n",
              "4       My stuffed animal I've had since I was born.  ...         NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmi_V7xDSa2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a935f85-a2bd-4039-ce89-08e1dc0465b5"
      },
      "source": [
        "yes_or_no.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 295218 entries, 0 to 295217\n",
            "Data columns (total 13 columns):\n",
            " #   Column       Non-Null Count   Dtype  \n",
            "---  ------       --------------   -----  \n",
            " 0   comment      295218 non-null  object \n",
            " 1   subreddit    295218 non-null  object \n",
            " 2   score        295218 non-null  int64  \n",
            " 3   sarcasm      295218 non-null  object \n",
            " 4   Llave        295218 non-null  object \n",
            " 5   Unnamed: 5   1 non-null       object \n",
            " 6   Unnamed: 6   10 non-null      object \n",
            " 7   Unnamed: 7   15 non-null      float64\n",
            " 8   Unnamed: 8   10 non-null      float64\n",
            " 9   Unnamed: 9   0 non-null       float64\n",
            " 10  Unnamed: 10  2 non-null       object \n",
            " 11  Unnamed: 11  3 non-null       float64\n",
            " 12  Unnamed: 12  2 non-null       float64\n",
            "dtypes: float64(5), int64(1), object(7)\n",
            "memory usage: 29.3+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wl1ibhUrcG5F",
        "outputId": "5cbda7c5-1a62-4392-9deb-b0bd08ec1dba"
      },
      "source": [
        "yes_or_no[\"subreddit\"].value_counts()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AskReddit          90658\n",
              "politics           64392\n",
              "worldnews          51288\n",
              "leagueoflegends    45979\n",
              "funny              42901\n",
              "Name: subreddit, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI_mpMTjckOc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75357a5-53f5-47af-ec4b-669a68fe78e2"
      },
      "source": [
        "cantAskRedditYes=len(yes_or_no[(yes_or_no['subreddit']==\"AskReddit\")& (yes_or_no['sarcasm']==\"yes\")])\n",
        "cantAskRedditNo=len(yes_or_no[(yes_or_no['subreddit']==\"AskReddit\")& (yes_or_no['sarcasm']==\"no\")])\n",
        "\n",
        "cantFunnyYes=len(yes_or_no[(yes_or_no['subreddit']==\"funny\")& (yes_or_no['sarcasm']==\"yes\")])\n",
        "cantFunnyNo=len(yes_or_no[(yes_or_no['subreddit']==\"funny\")& (yes_or_no['sarcasm']==\"no\")])\n",
        "\n",
        "cantPoliticsYes=len(yes_or_no[(yes_or_no['subreddit']==\"politics\")& (yes_or_no['sarcasm']==\"yes\")])\n",
        "cantPoliticsNo=len(yes_or_no[(yes_or_no['subreddit']==\"politics\")& (yes_or_no['sarcasm']==\"no\")])\n",
        "\n",
        "cantLeagueOfLegendsYes=len(yes_or_no[(yes_or_no['subreddit']==\"leagueoflegends\")& (yes_or_no['sarcasm']==\"yes\")])\n",
        "cantLeagueOfLegendsNo=len(yes_or_no[(yes_or_no['subreddit']==\"leagueoflegends\")& (yes_or_no['sarcasm']==\"no\")])\n",
        "\n",
        "cantWorldNewsYes=len(yes_or_no[(yes_or_no['subreddit']==\"worldnews\")& (yes_or_no['sarcasm']==\"yes\")])\n",
        "cantWorldNewsNo=len(yes_or_no[(yes_or_no['subreddit']==\"worldnews\")& (yes_or_no['sarcasm']==\"no\")])\n",
        "\n",
        "cantAskReddit=cantAskRedditYes+cantAskRedditNo\n",
        "cantFunny=cantFunnyYes+cantFunnyNo\n",
        "cantPolitics=cantPoliticsYes+cantPoliticsNo\n",
        "cantLeagueOfLegends=cantLeagueOfLegendsYes+cantLeagueOfLegendsNo\n",
        "cantWorldNews=cantWorldNewsYes+cantWorldNewsNo\n",
        "\n",
        "print(\"Proporción comentarios sarcasticos en AskReddit: \", cantAskRedditYes/cantAskReddit)\n",
        "print(\"Proporción comentarios no sarcasticos en AskReddit: \", cantAskRedditNo/cantAskReddit)\n",
        "\n",
        "print(\"Proporción comentarios sarcasticos en funny: \", cantFunnyYes/cantFunny)\n",
        "print(\"Proporción comentarios no sarcasticos en funn: \", cantFunnyNo/cantFunny)\n",
        "\n",
        "print(\"Proporción comentarios sarcasticos en politics: \", cantPoliticsYes/cantPolitics)\n",
        "print(\"Proporción comentarios no sarcasticos en politics: \", cantPoliticsNo/cantPolitics)\n",
        "\n",
        "print(\"Proporción comentarios sarcasticos en leagueoflegends: \", cantLeagueOfLegendsYes/cantLeagueOfLegends)\n",
        "print(\"Proporción comentarios no sarcasticos en leagueoflegends: \", cantLeagueOfLegendsNo/cantLeagueOfLegends)\n",
        "\n",
        "print(\"Proporción comentarios sarcasticos en worldnews: \", cantWorldNewsYes/cantWorldNews)\n",
        "print(\"Proporción comentarios no sarcasticos en worldnews: \", cantWorldNewsNo/cantWorldNews)\n",
        "\n",
        "\n",
        "cantSarcastica=cantAskRedditYes+cantFunnyYes+cantPoliticsYes+cantLeagueOfLegendsYes+cantWorldNewsYes\n",
        "cantNotSarcastica=cantAskRedditNo+cantFunnyNo+cantPoliticsNo+cantLeagueOfLegendsNo+cantWorldNewsNo\n",
        "cantTotal=cantSarcastica+cantNotSarcastica\n",
        "\n",
        "print(\"Proporción comentarios sarcasticos en el dataset: \", cantSarcastica/cantTotal)\n",
        "print(\"Proporción comentarios no sarcasticos en el dataset: \", cantNotSarcastica/cantTotal)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Proporción comentarios sarcasticos en AskReddit:  0.7244148337708751\n",
            "Proporción comentarios no sarcasticos en AskReddit:  0.2755851662291248\n",
            "Proporción comentarios sarcasticos en funny:  0.41814876110113985\n",
            "Proporción comentarios no sarcasticos en funn:  0.5818512388988601\n",
            "Proporción comentarios sarcasticos en politics:  0.6133215306249223\n",
            "Proporción comentarios no sarcasticos en politics:  0.38667846937507766\n",
            "Proporción comentarios sarcasticos en leagueoflegends:  0.4574697144348507\n",
            "Proporción comentarios no sarcasticos en leagueoflegends:  0.5425302855651493\n",
            "Proporción comentarios sarcasticos en worldnews:  0.5142723444080487\n",
            "Proporción comentarios no sarcasticos en worldnews:  0.48572765559195136\n",
            "Proporción comentarios sarcasticos en el dataset:  0.5775935071709719\n",
            "Proporción comentarios no sarcasticos en el dataset:  0.42240649282902804\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H0wh8mJ_pme"
      },
      "source": [
        "# Vectorización"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDctXx4_uvW"
      },
      "source": [
        "**Tokenización**: convertir un párrafo u oración a unidades (tokens), usualmente cada palabra es un token. \n",
        "\n",
        "En este caso, nuestra función `tokenize` es bastante simple (e ineficiente), pero sirve para nuestros simple propósito.\n",
        "\n",
        "**Stopword removal**: eliminar tokens irrelevantes, palabras comunes y a veces signos de puntuación.\n",
        "\n",
        "En nuestro caso, únicamente estamos eliminando los símbolos de puntuación con ayuda del set `punctuation`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8zK5n84mohU",
        "outputId": "81ca61b1-df04-40ba-8946-14a59eb41439"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "print(stopwords.words('english'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7aZ7JDN0gyc"
      },
      "source": [
        "import string\n",
        "punctuation = set(string.punctuation)\n",
        "digits = set(string.digits)\n",
        "words=stopwords.words('english')\n",
        "\n",
        "def tokenize(sentence):\n",
        "    tokens = []\n",
        "    for token in sentence.split():\n",
        "        new_token = []\n",
        "        #if (token not in words):\n",
        "        for character in token:\n",
        "            if (character not in punctuation) and (character not in digits) :\n",
        "                new_token.append(character.lower())\n",
        "        if new_token:\n",
        "            tokens.append(\"\".join(new_token))\n",
        "    return tokens"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfN4KFyw89ke",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b8d521-27e4-46cd-d949-04a666aca482"
      },
      "source": [
        "tokenize(\"Go until jurong point, crazy, i love you 1313\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['go', 'until', 'jurong', 'point', 'crazy', 'i', 'love', 'you']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veDcXo2y-hfm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187d2952-7624-4368-f758-7b2203848950"
      },
      "source": [
        "yes_or_no.head()[\"comment\"].apply(tokenize)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    [i, dont, pay, attention, to, her, but, as, lo...\n",
              "1    [trick, or, treating, in, general, is, just, w...\n",
              "2                                    [what, the, fuck]\n",
              "3                         [this, would, make, me, cry]\n",
              "4    [my, stuffed, animal, ive, had, since, i, was,...\n",
              "Name: comment, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKo6528kD-Fe"
      },
      "source": [
        "**Stemming/Lemmatization**: Convertir cada token a su forma base: {“biblioteca”, “bibliotecario”, ”bibliotecas”} → “bibliotec”.\n",
        "\n",
        "En nuestro caso no estamos haciendo este paso, pero si es necesario, puedes revisar cosas como [NLTK - stemming](https://pythonspot.com/nltk-stemming/) o [Lemmatization Approaches with Examples in Python](https://www.machinelearningplus.com/nlp/lemmatization-examples-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aL-Bd2PRZNy"
      },
      "source": [
        "**One-Hot encoding**: después de la tokenización, poner en una tabla todos los tokens en el vocabulario y por cada ocurrencia de un token en un texto, marcar con un 1 en la fila correspondiente, por ejemplo considerando las dos frases siguientes:\n",
        "\n",
        " 1. Call FREEPHONE 0800 542 0578 now!\n",
        " 2. Did you call me just now ah?\n",
        " \n",
        "Obtendríamos algo como esto:\n",
        " \n",
        "|       | 0578 | 0800 | 542 | ah | call | did | freephone | just | me | now | you |\n",
        "|-------|------|------|-----|----|------|-----|-----------|------|----|-----|-----|\n",
        "| **1** | 1    | 1    | 1   | 0  | 1    | 0   | 1         | 0    | 0  | 1   | 0   |\n",
        "| **2** | 0    | 0    | 0   | 1  | 1    | 1   | 0         | 1    | 1  | 1   | 1   |  \n",
        "\n",
        "Aquí es donde entra **Scikit-Learn** a través de la clase `CountVectorizer` del módulo `sklearn.feature_extraction.text`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pdy1qPioUXE"
      },
      "source": [
        "[Slides]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uW6Pnb0_USg"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-zQf0fOirNC"
      },
      "source": [
        "# Pequeño Ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b8sjRK5Uy-X"
      },
      "source": [
        "demo_vectorizer = CountVectorizer(\n",
        "    tokenizer = tokenize,\n",
        "    binary=True\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdCTha5gd_J"
      },
      "source": [
        "Explicación de los parámetros:  \n",
        "\n",
        " - **tokenizer = tokenize**: `CountVectorizer` tiene un tokenizador por default, al pasarle nuestra función lo estamos reemplazando con el que nosotros escribimos.  \n",
        " - **binary = True**: `CountVectorizer` por default en lugar de `1` cuenta el número de ocurrencias de cada token, al establecer `binary = True`, le estamos indicando que no importa cuantas veces ocurra una palabra, solamente la debe contar una vez"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fk4yppAjXoOM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f774e885-7305-4127-bc31-aae6fa9c3412"
      },
      "source": [
        "examples = [\n",
        "    \"Call FREEPHONE 0800 542 0578 now!\",\n",
        "    \"Did you call me just now ah?\"\n",
        "]\n",
        "demo_vectorizer.fit(examples)\n",
        "vectors = demo_vectorizer.transform(examples).toarray()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkQRaxBCiLIS"
      },
      "source": [
        "Usamos `fit` y `transform` de manera separada, aunque en este caso pudimos haber usado `fit_transform`.\n",
        "\n",
        "**Nota**: usamos `toarray` para obtener un un *numpy array* ya que por default `transform` devuelve una [matriz dispersa](https://en.wikipedia.org/wiki/Sparse_matrix) que, mientras que es buena para no consumir memoria, no es tan amigable para mostrar cómo es que se ven los datos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29hOfmHeYJf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "e1909db4-f739-4bed-e87a-9e3cfacbb3f2"
      },
      "source": [
        "headers = sorted(demo_vectorizer.vocabulary_.keys())\n",
        "pd.DataFrame(vectors, columns=headers)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ah</th>\n",
              "      <th>call</th>\n",
              "      <th>did</th>\n",
              "      <th>freephone</th>\n",
              "      <th>just</th>\n",
              "      <th>me</th>\n",
              "      <th>now</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ah  call  did  freephone  just  me  now  you\n",
              "0   0     1    0          1     0   0    1    0\n",
              "1   1     1    1          0     1   1    1    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYA663KfixKs"
      },
      "source": [
        "#Resultado preliminar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBmSSn5nkSh9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdDQ5yOPjx_F"
      },
      "source": [
        "## Clasificadores para AskReddit\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3cljWXPLFsN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "bfaad986-3d3d-4d82-b881-1db5d8638363"
      },
      "source": [
        "\n",
        "datosAskReddit=yes_or_no[yes_or_no['subreddit']==\"AskReddit\"]\n",
        "datosAskReddit.head()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>Llave</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I don't pay attention to her, but as long as s...</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td></td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>65674.0</td>\n",
              "      <td>0.724415</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sarcasm</td>\n",
              "      <td>170516.0</td>\n",
              "      <td>0.577594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Trick or treating in general is just weird...</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>AskRedditno</td>\n",
              "      <td>24984.0</td>\n",
              "      <td>0.275585</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nosarcasm</td>\n",
              "      <td>124702.0</td>\n",
              "      <td>0.422406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>what the fuck</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>22</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>90658.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>295218.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This would make me cry.</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>My stuffed animal I've had since I was born.</td>\n",
              "      <td>AskReddit</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>AskReddityes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>funnyyes</td>\n",
              "      <td>17939.0</td>\n",
              "      <td>0.418149</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             comment  ... Unnamed: 12\n",
              "0  I don't pay attention to her, but as long as s...  ...    0.577594\n",
              "1      Trick or treating in general is just weird...  ...    0.422406\n",
              "2                                      what the fuck  ...         NaN\n",
              "3                            This would make me cry.  ...         NaN\n",
              "4       My stuffed animal I've had since I was born.  ...         NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk5jKAdtwF5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1db6c92-45a1-41a5-f68d-193a4c9afe05"
      },
      "source": [
        "train_comment_A,test_comment_A, train_sarcasm_A, test_sarcasm_A = train_test_split(datosAskReddit[\"comment\"], \n",
        "                                                                    datosAskReddit[\"sarcasm\"],\n",
        "                                                                    test_size=.33, random_state=37,\n",
        "                                                                    stratify=datosAskReddit[\"sarcasm\"])\n",
        "\n",
        "print(f\"Training examples: {len(train_comment_A)}, testing examples {len(test_comment_A)}\")\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training examples: 60740, testing examples 29918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTE1NIZ5cmZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3afff886-fa15-412f-95d9-7a1d319b5d89"
      },
      "source": [
        "real_vectorizer = CountVectorizer(tokenizer = tokenize, binary=True)\n",
        "train_X_A = real_vectorizer.fit_transform(train_comment_A)\n",
        "test_X_A = real_vectorizer.transform(test_comment_A)\n",
        "train_X_A.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60740, 39822)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1TqRuadavKI"
      },
      "source": [
        "###Usando Arbol de decision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCFMpCooYZYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9d0051b-140f-4aab-8010-2fbc407a440b"
      },
      "source": [
        "classifier_Tree_Ask = DecisionTreeClassifier()\n",
        "classifier_Tree_Ask.fit(train_X_A, train_sarcasm_A)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z39iEx8Ta49W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b724fb77-a63c-4a1a-ba3a-3b65c84c82e8"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predicciones_Tree_Ask = classifier_Tree_Ask.predict(test_X_A)\n",
        "accuracy_Tree_Ask = accuracy_score(test_sarcasm_A, predicciones_Tree_Ask)\n",
        "print(f\"Accuracy: {accuracy_Tree_Ask:.4%}\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 76.1314%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSGmOZgcqunv"
      },
      "source": [
        "###Usando linearSvc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cU-VbdYrJys",
        "outputId": "a8bf0d7a-4e66-4807-c00b-176f09ecb914"
      },
      "source": [
        "classifier_Svc_Ask = LinearSVC( max_iter=300000)\n",
        "classifier_Svc_Ask.fit(train_X_A, train_sarcasm_A)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=300000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-aFcZDbrTtx",
        "outputId": "3495bde8-49e4-432a-cab1-f37c33515155"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predicciones_Svc_Ask = classifier_Svc_Ask.predict(test_X_A)\n",
        "accuracy_Svc_Ask = accuracy_score(test_sarcasm_A, predicciones_Svc_Ask)\n",
        "print(f\"Accuracy: {accuracy_Svc_Ask:.4%}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 80.3764%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYKEhnV7sdHv"
      },
      "source": [
        "## Clasificadores para politics\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "KX4k9wAosdHw",
        "outputId": "2d9bc9fc-27c1-4ed6-c5e7-fee7495bb34f"
      },
      "source": [
        "\n",
        "datosPolitics=yes_or_no[yes_or_no['subreddit']==\"politics\"]\n",
        "datosPolitics.head()\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>score</th>\n",
              "      <th>sarcasm</th>\n",
              "      <th>Llave</th>\n",
              "      <th>Unnamed: 5</th>\n",
              "      <th>Unnamed: 6</th>\n",
              "      <th>Unnamed: 7</th>\n",
              "      <th>Unnamed: 8</th>\n",
              "      <th>Unnamed: 9</th>\n",
              "      <th>Unnamed: 10</th>\n",
              "      <th>Unnamed: 11</th>\n",
              "      <th>Unnamed: 12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>104647</th>\n",
              "      <td>NC and NH.</td>\n",
              "      <td>politics</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>politicsyes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104648</th>\n",
              "      <td>I think a significant amount would be against ...</td>\n",
              "      <td>politics</td>\n",
              "      <td>92</td>\n",
              "      <td>yes</td>\n",
              "      <td>politicsyes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104649</th>\n",
              "      <td>because it's what really bothers him... and it...</td>\n",
              "      <td>politics</td>\n",
              "      <td>15</td>\n",
              "      <td>yes</td>\n",
              "      <td>politicsyes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104650</th>\n",
              "      <td>Conservatism as an ideology is for sure a reac...</td>\n",
              "      <td>politics</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>politicsyes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104651</th>\n",
              "      <td>Maybe not control, but certainly that is evide...</td>\n",
              "      <td>politics</td>\n",
              "      <td>1</td>\n",
              "      <td>yes</td>\n",
              "      <td>politicsyes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  comment  ... Unnamed: 12\n",
              "104647                                         NC and NH.  ...         NaN\n",
              "104648  I think a significant amount would be against ...  ...         NaN\n",
              "104649  because it's what really bothers him... and it...  ...         NaN\n",
              "104650  Conservatism as an ideology is for sure a reac...  ...         NaN\n",
              "104651  Maybe not control, but certainly that is evide...  ...         NaN\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E658E-6bsdHx",
        "outputId": "f4d33b89-180f-4b64-d2b7-6ad91f0f2ab7"
      },
      "source": [
        "train_comment_P,test_comment_P, train_sarcasm_P, test_sarcasm_P = train_test_split(datosPolitics[\"comment\"], \n",
        "                                                                    datosPolitics[\"sarcasm\"],\n",
        "                                                                    test_size=.33, random_state=37,\n",
        "                                                                    stratify=datosPolitics[\"sarcasm\"])\n",
        "\n",
        "print(f\"Training examples: {len(train_comment_P)}, testing examples {len(test_comment_P)}\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training examples: 43142, testing examples 21250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCAbeh1KsdHy",
        "outputId": "4c9c3f95-bf78-4919-dfe4-2c0fea3c87c3"
      },
      "source": [
        "real_vectorizer = CountVectorizer(tokenizer = tokenize, binary=True)\n",
        "\n",
        "train_X_P = real_vectorizer.fit_transform(train_comment_P)\n",
        "test_X_P = real_vectorizer.transform(test_comment_P)\n",
        "\n",
        "train_X_P.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43142, 30975)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6YRcR44sdHy"
      },
      "source": [
        "###Usando Arbol de decision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQFZ3_tisdHy",
        "outputId": "c1b74ddf-f2e8-416b-9c6e-97ea2e54be90"
      },
      "source": [
        "classifier_Tree_P = DecisionTreeClassifier()\n",
        "classifier_Tree_P.fit(train_X_P, train_sarcasm_P)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4pMpkTusdHz",
        "outputId": "4d43d151-0e75-4d38-a78d-8bb1aca8f52f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predicciones_Tree_P = classifier_Tree_P.predict(test_X_P)\n",
        "accuracy_Tree_P = accuracy_score(test_sarcasm_P, predicciones_Tree_P)\n",
        "print(f\"Accuracy: {accuracy_Tree_P:.4%}\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 75.0635%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scb4tK_AsdHz"
      },
      "source": [
        "###Usando linearSvc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufjn1d0JsdHz",
        "outputId": "8ca667b4-79cd-4ae0-84fc-32ef8178ac09"
      },
      "source": [
        "classifier_Svc_P = LinearSVC( max_iter=300000)\n",
        "classifier_Svc_P.fit(train_X_P, train_sarcasm_P)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=300000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaWqqZ2ysdHz",
        "outputId": "415f289d-d141-4c76-d59b-a80800ee4f79"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "predicciones_Svc_P = classifier_Svc_P.predict(test_X_P)\n",
        "accuracy_Svc_P = accuracy_score(test_sarcasm_P, predicciones_Svc_P)\n",
        "print(f\"Accuracy: {accuracy_Svc_P:.4%}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 81.8871%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMRLfOR5t9iz"
      },
      "source": [
        "# Pregunta 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6AFezaxvORd"
      },
      "source": [
        "### Usando LinearSvc con askreddit en entramiento y polits pa testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-WVthlnvjrJ",
        "outputId": "e26bdb48-78a3-485b-cc0e-981d118fda50"
      },
      "source": [
        "print(test_X_P.shape)\n",
        "print(test_X_A.shape)\n",
        "\n",
        "predicciones_PA = classifier_Svc_P.predict(test_X_A[:,0:30975])\n",
        "accuracy_PA = accuracy_score(test_sarcasm_A, predicciones_PA)\n",
        "print(f\"Accuracy: {accuracy_PA:.4%}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(21250, 30975)\n",
            "(29918, 39822)\n",
            "Accuracy: 69.9512%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cLoGy-xsdH0"
      },
      "source": [
        "### Predicciones en nuevos datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lL1TJiKrsdH0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "b2d7d75b-2eba-4ddc-bb14-d2db6eb00a0e"
      },
      "source": [
        "examples=[\"the penguin loves nicolas\",\n",
        "          \"I love my life and my dog\",\n",
        "          \"i like the idea but not the execution\",\n",
        "          \"the sky is blue\",\n",
        "          \"great idea\",\n",
        "          \"chile has many earthquakes\",\n",
        "          \"i was looking for this comment\",\n",
        "          \"my mother died yesterday\",\n",
        "          \"now I have anal cancer\",\n",
        "          \"trump is the worst president of USA\",\n",
        "          \"Republicans will take credit once it's done.\",\n",
        "          \"Fuck this\",\n",
        "          \"the huracan maria destroyed puerto rico\",\n",
        "          \"Hilarious that Russia’s statements intended to back up Belarus basically say that it is exactly what it looks like, but no big deal.\"\n",
        "          ]\n",
        "#for i in test_text:\n",
        " # examples.append(i)\n",
        "\n",
        "examples_X = real_vectorizer.transform(examples)\n",
        "predicciones = classifier.predict(examples_X)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-8f5a067c795d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mexamples_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpredicciones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAj7q0ndsdH0"
      },
      "source": [
        "for text, label in zip(examples, predicciones):\n",
        "    print(f\"{label:5} - {text}\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}